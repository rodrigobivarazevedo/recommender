{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e578d097",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58ff8886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "data_path = \"embedded_podcast_data.pkl\"\n",
    "\n",
    "with open(data_path, \"rb\") as f:\n",
    "    embedded_df  = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c704efa0",
   "metadata": {},
   "source": [
    "# Transformer Filtering Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e544579",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cosine_similarity_matrix_raw(vec, matrix):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between a single vector `vec` and each row vector in `matrix`.\n",
    "    vec: 1D numpy array, shape (d,)\n",
    "    matrix: 2D numpy array, shape (n, d)\n",
    "    Returns:\n",
    "        similarities: 1D numpy array, shape (n,)\n",
    "    \"\"\"\n",
    "    # Normalize the input vector and matrix row-wise\n",
    "    vec_norm = vec / np.linalg.norm(vec)\n",
    "    matrix_norm = matrix / np.linalg.norm(matrix, axis=1, keepdims=True)\n",
    "    \n",
    "    # Dot product between vec and each row in matrix (broadcasting)\n",
    "    similarities = np.dot(matrix_norm, vec_norm)\n",
    "    return similarities\n",
    "\n",
    "def cosine_similarity_matrix_normalized(vec, matrix):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between normalized vector `vec` and each normalized row in `matrix`.\n",
    "    Inputs are assumed normalized.\n",
    "    \"\"\"\n",
    "    return np.dot(matrix, vec)\n",
    "\n",
    "def filtering(user_input, original_df, model, max_min=None, top_n=100, column='metadata_embedding'):\n",
    "    \"\"\"\n",
    "    Filter DataFrame based on length of episodes and return top_n most similar.\n",
    "    \"\"\"\n",
    "    df = original_df.copy()\n",
    "    \n",
    "    if max_min is not None:\n",
    "        df = df[df['duration_min'] <= max_min]\n",
    "\n",
    "    # Encode user input\n",
    "    user_emb = model.encode(user_input)\n",
    "    \n",
    "    user_emb_norm = normalize( user_emb.reshape(1, -1))[0]\n",
    "\n",
    "    # Stack embeddings into a numpy matrix\n",
    "    normalized_embeddings_matrix = np.vstack(df[column].values)\n",
    "\n",
    "    # Compute all cosine similarities at once\n",
    "    similarities = cosine_similarity_matrix_normalized(user_emb_norm, normalized_embeddings_matrix)\n",
    "\n",
    "    # Add similarity scores\n",
    "    df = df.assign(similarity=similarities)\n",
    "\n",
    "    # Get top_n most similar rows\n",
    "    top = df.sort_values(by=\"similarity\", ascending=False).head(top_n)\n",
    "\n",
    "    # Display top 5 results\n",
    "    for idx, row in top.iloc[:5].iterrows():\n",
    "        print(f\"\\nðŸŽ¯ Title: {row['title']}\")\n",
    "        print(f\"ðŸŽ™ï¸ Host: {row['host']}\")\n",
    "        print(f\"ðŸ§  Similarity: {row['similarity']:.4f}\")\n",
    "\n",
    "    return top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17f23483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¯ Title: Dr. Kyle Gillett How to Optimize Your Hormones for Health & Vitality\n",
      "ðŸŽ™ï¸ Host: huberman\n",
      "ðŸ§  Similarity: 0.5942\n",
      "\n",
      "ðŸŽ¯ Title: Dr. Sara Gottfried How to Optimize Female Hormone Health for Vitality & Longevity | Huberman Lab\n",
      "ðŸŽ™ï¸ Host: huberman\n",
      "ðŸ§  Similarity: 0.5732\n",
      "\n",
      "ðŸŽ¯ Title: How to Optimize Fertility in Males & Females\n",
      "ðŸŽ™ï¸ Host: huberman\n",
      "ðŸ§  Similarity: 0.5581\n",
      "\n",
      "ðŸŽ¯ Title: Dr. Stacy Sims Female-Specific Exercise & Nutrition for Health, Performance & Longevity\n",
      "ðŸŽ™ï¸ Host: huberman\n",
      "ðŸ§  Similarity: 0.5191\n",
      "\n",
      "ðŸŽ¯ Title: Most Efficient Way for Women to Train for Overall Fitness | Dr. Stacy Sims & Dr. Andrew Huberman\n",
      "ðŸŽ™ï¸ Host: huberman\n",
      "ðŸ§  Similarity: 0.5129\n"
     ]
    }
   ],
   "source": [
    "# User input expressing a goal about sleep and anxiety\n",
    "user_input = \"optimizing my overall female health.\"\n",
    "\n",
    "# load the model\n",
    "model = SentenceTransformer(\"models/embedding_model\")\n",
    "\n",
    "prefiltered_df = filtering(user_input, embedded_df, model, max_min=None, top_n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b72b647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¯ Title: How to Optimize Testosterone & Estrogen | Huberman Lab Essentials\n",
      "ðŸŽ™ï¸ Host: huberman\n",
      "ðŸ§  Similarity: 0.4783\n",
      "\n",
      "ðŸŽ¯ Title: Dr. Kyle Gillett How to Optimize Your Hormones for Health & Vitality\n",
      "ðŸŽ™ï¸ Host: huberman\n",
      "ðŸ§  Similarity: 0.4717\n",
      "\n",
      "ðŸŽ¯ Title: The Science of How to Optimize Testosterone & Estrogen\n",
      "ðŸŽ™ï¸ Host: huberman\n",
      "ðŸ§  Similarity: 0.4707\n",
      "\n",
      "ðŸŽ¯ Title: How to Control Your Metabolism by Thyroid & Growth Hormone\n",
      "ðŸŽ™ï¸ Host: huberman\n",
      "ðŸ§  Similarity: 0.4511\n",
      "\n",
      "ðŸŽ¯ Title: Testosterone & Testosterone Replacement Therapy (TRT) | Dr. Peter Attia & Dr. Andrew Huberman\n",
      "ðŸŽ™ï¸ Host: huberman\n",
      "ðŸ§  Similarity: 0.4455\n"
     ]
    }
   ],
   "source": [
    "# User input expressing a goal about sleep and anxiety\n",
    "user_input = \"What can I do to balance hormones naturally?\"\n",
    "\n",
    "# load the model\n",
    "model = SentenceTransformer(\"models/embedding_model\")\n",
    "\n",
    "prefiltered_df = filtering(user_input, embedded_df, model, max_min=None, top_n=100, column='transcript_embedding_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf52971e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¯ Title: Developing a Rational Approach to Supplementation for Health & Performance | Huberman Lab Podcast\n",
      "ðŸŽ™ï¸ Host: huberman\n",
      "ðŸ§  Similarity: 0.4531\n",
      "\n",
      "ðŸŽ¯ Title: Maximize Productivity, Physical & Mental Health With Daily Tools | Huberman Lab Essentials\n",
      "ðŸŽ™ï¸ Host: huberman\n",
      "ðŸ§  Similarity: 0.4441\n",
      "\n",
      "ðŸŽ¯ Title: Using Caffeine to Optimize Mental & Physical Performance | Huberman Lab Podcast 101\n",
      "ðŸŽ™ï¸ Host: huberman\n",
      "ðŸ§  Similarity: 0.4427\n",
      "\n",
      "ðŸŽ¯ Title: Using Cortisol & Adrenaline to Boost Our Energy & Immune System Function\n",
      "ðŸŽ™ï¸ Host: huberman\n",
      "ðŸ§  Similarity: 0.4359\n",
      "\n",
      "ðŸŽ¯ Title: Understanding & Conquering Depression | Huberman Lab Essentials\n",
      "ðŸŽ™ï¸ Host: huberman\n",
      "ðŸ§  Similarity: 0.4327\n"
     ]
    }
   ],
   "source": [
    "# User input expressing a goal about sleep and anxiety\n",
    "user_input = \"What supplements can help reduce PMS symptoms like fatigue and mood swings?\"\n",
    "\n",
    "# load the model\n",
    "model = SentenceTransformer(\"models/embedding_model\")\n",
    "\n",
    "prefiltered_df = filtering(user_input, embedded_df, model, max_min=None, top_n=100, column='transcript_embedding_weighted_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61b75157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¯ Title: Controlling Your Dopamine For Motivation, Focus & Satisfaction\n",
      "ðŸŽ™ï¸ Host: huberman\n",
      "ðŸ§  Similarity: 0.6217\n",
      "\n",
      "ðŸŽ¯ Title: How to Increase Motivation & Drive | Huberman Lab Essentials\n",
      "ðŸŽ™ï¸ Host: huberman\n",
      "ðŸ§  Similarity: 0.5874\n",
      "\n",
      "ðŸŽ¯ Title: Dopamine Baseline, Impulsivity  & Addiction | Dr. Anna Lempke & Dr.Andrew Huberman\n",
      "ðŸŽ™ï¸ Host: huberman\n",
      "ðŸ§  Similarity: 0.5691\n",
      "\n",
      "ðŸŽ¯ Title: How to Increase Motivation & Drive\n",
      "ðŸŽ™ï¸ Host: huberman\n",
      "ðŸ§  Similarity: 0.5659\n",
      "\n",
      "ðŸŽ¯ Title: Leverage Dopamine to Overcome Procrastination & Optimize Effort | Huberman Lab Podcast\n",
      "ðŸŽ™ï¸ Host: huberman\n",
      "ðŸ§  Similarity: 0.5551\n"
     ]
    }
   ],
   "source": [
    "# User input expressing a goal about sleep and anxiety\n",
    "user_input = \"i want to learn about dopamine and its effects on the brain\"\n",
    "\n",
    "# load the model\n",
    "model = SentenceTransformer(\"models/embedding_model\")\n",
    "\n",
    "prefiltered_df = filtering(user_input, embedded_df, model, max_min=None, top_n=100, column='transcript_embedding_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9e8d85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¯ Title: The Science of How to Optimize Testosterone & Estrogen\n",
      "ðŸŽ™ï¸ Host: huberman\n",
      "ðŸ§  Similarity: 0.4579\n",
      "\n",
      "ðŸŽ¯ Title: How to Optimize Testosterone & Estrogen | Huberman Lab Essentials\n",
      "ðŸŽ™ï¸ Host: huberman\n",
      "ðŸ§  Similarity: 0.4361\n",
      "\n",
      "ðŸŽ¯ Title: Dr. Kyle Gillett How to Optimize Your Hormones for Health & Vitality\n",
      "ðŸŽ™ï¸ Host: huberman\n",
      "ðŸ§  Similarity: 0.3961\n",
      "\n",
      "ðŸŽ¯ Title: How to Optimize Fertility in Males & Females\n",
      "ðŸŽ™ï¸ Host: huberman\n",
      "ðŸ§  Similarity: 0.3949\n",
      "\n",
      "ðŸŽ¯ Title: Dr. Sara Gottfried How to Optimize Female Hormone Health for Vitality & Longevity | Huberman Lab\n",
      "ðŸŽ™ï¸ Host: huberman\n",
      "ðŸ§  Similarity: 0.3881\n"
     ]
    }
   ],
   "source": [
    "# User input expressing a goal about sleep and anxiety\n",
    "user_input = \"how to balance my progesterone levels\"\n",
    "\n",
    "# load the model\n",
    "model = SentenceTransformer(\"models/embedding_model\")\n",
    "\n",
    "prefiltered_df = filtering(user_input, embedded_df, model, max_min=None, top_n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0564e678",
   "metadata": {},
   "source": [
    "# Using TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0fc0126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorizers_filtering(user_input, original_df, vectorizer, max_min=None, top_n=100, column='transcript_embedding_weighted_mean_TfidfVectorizer'):\n",
    "    df = original_df.copy()\n",
    "\n",
    "    if max_min is not None:\n",
    "        df = df[df['duration_min'] <= max_min]\n",
    "\n",
    "    # Vectorize user input and normalize\n",
    "    user_emb_sparse = vectorizer.transform([user_input])\n",
    "    user_emb = normalize(user_emb_sparse).toarray()[0]\n",
    "\n",
    "    # Filter out rows with None embeddings\n",
    "    df = df[df[column].apply(lambda x: x is not None)]\n",
    "\n",
    "    embeddings_matrix = np.vstack(df[column].values)\n",
    "    embeddings_matrix_norm = embeddings_matrix / np.linalg.norm(embeddings_matrix, axis=1, keepdims=True)\n",
    "\n",
    "    similarities = np.dot(embeddings_matrix_norm, user_emb)\n",
    "\n",
    "    df = df.assign(similarity=similarities)\n",
    "    top = df.sort_values(by=\"similarity\", ascending=False).head(top_n)\n",
    "\n",
    "    for idx, row in top.iloc[:5].iterrows():\n",
    "        print(f\"\\nðŸŽ¯ Title: {row['title']}\")\n",
    "        print(f\"ðŸŽ™ï¸ Host: {row['host']}\")\n",
    "        print(f\"ðŸ§  Similarity: {row['similarity']:.4f}\")\n",
    "\n",
    "    return top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "91b61acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "data_path = \"TfidfVectorizer_embedded_podcast_data.pkl\"\n",
    "\n",
    "with open(data_path, \"rb\") as f:\n",
    "    TfidfVectorizer_embedded_df  = pickle.load(f)\n",
    "    \n",
    "    # Load vectorizer\n",
    "with open('vectorizers/metadata_vectorizer.pkl', 'rb') as f:\n",
    "    metadata_vectorizer = pickle.load(f)\n",
    "\n",
    "with open('vectorizers/chunk_vectorizer.pkl', 'rb') as f:\n",
    "    chunk_vectorizer = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cbae58d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¯ Title: How to Control Your Metabolism by Thyroid & Growth Hormone | Huberman Lab Essentials\n",
      "ðŸŽ™ï¸ Host: huberman\n",
      "ðŸ§  Similarity: 0.4053\n",
      "\n",
      "ðŸŽ¯ Title: How to Control Your Metabolism by Thyroid & Growth Hormone\n",
      "ðŸŽ™ï¸ Host: huberman\n",
      "ðŸ§  Similarity: 0.3484\n",
      "\n",
      "ðŸŽ¯ Title: Developing a Rational Approach to Supplementation for Health & Performance | Huberman Lab Podcast\n",
      "ðŸŽ™ï¸ Host: huberman\n",
      "ðŸ§  Similarity: 0.2703\n",
      "\n",
      "ðŸŽ¯ Title: The Science of How to Optimize Testosterone & Estrogen\n",
      "ðŸŽ™ï¸ Host: huberman\n",
      "ðŸ§  Similarity: 0.2638\n",
      "\n",
      "ðŸŽ¯ Title: Dr. Kyle Gillett How to Optimize Your Hormones for Health & Vitality\n",
      "ðŸŽ™ï¸ Host: huberman\n",
      "ðŸ§  Similarity: 0.2632\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# User input expressing a goal about sleep and anxiety\n",
    "user_input = \"I want to optimise my hormone health\"\n",
    "\n",
    "prefiltered_df = vectorizers_filtering(user_input, TfidfVectorizer_embedded_df, chunk_vectorizer, max_min=None, top_n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ff12842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0__transcript.txt\n",
      "0__transcript.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import string\n",
    "\n",
    "df = pd.read_csv('youtube_extract_Andrew_Huberman.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Load the list of video IDs\n",
    "with open('videos.txt', 'r', encoding='utf-8') as f:\n",
    "    video_ids = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# Create a mapping from video ID to transcript file path\n",
    "id_to_transcript_path = {\n",
    "    video_id: f\"{index}__transcript.txt\"\n",
    "    for index, video_id in enumerate(video_ids)\n",
    "}\n",
    "\n",
    "# Example: get path for video ID 'cp9GXl9Qk_s'\n",
    "print(id_to_transcript_path['cp9GXl9Qk_s'])  # Output: '0__transcript.txt'\n",
    "\n",
    "# Example: get transcript path for index 0\n",
    "print(f\"{0}__transcript.txt\")  # Same as above\n",
    "\n",
    "\n",
    "df[\"Transcript\"] = df[\"Video ID\"].map(id_to_transcript_path)\n",
    "df[\"Duration\"] = df[\"Duration\"] / 60  # Convert seconds to minutes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Optionally remove digits (if numbers are not useful)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def recommend_transcripts(user_input, top_n=5):\n",
    "\n",
    "    # Step 1: Load all .txt transcripts in the directory\n",
    "    transcript_dir = 'transcripts/transcripts-all'\n",
    "    transcript_paths = [\n",
    "        os.path.join(transcript_dir, fname)\n",
    "        for fname in os.listdir(transcript_dir)\n",
    "        if fname.endswith('.txt')\n",
    "    ]\n",
    "\n",
    "    transcripts = []\n",
    "    doc_names = []\n",
    "    \n",
    "    for path in transcript_paths:\n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "\n",
    "            # Split into sentences and filter short ones\n",
    "            sentences = re.split(r'(?<=[\\.\\?\\!])\\s+', text)\n",
    "            sentences = [s for s in sentences if len(s.strip()) > 20]\n",
    "\n",
    "            # Join and preprocess\n",
    "            reduced_text = preprocess(\" \".join(sentences))\n",
    "\n",
    "            transcripts.append(reduced_text)\n",
    "            doc_names.append(os.path.basename(path))\n",
    "\n",
    "\n",
    "    # Step 3: TF-IDF vectorization for similarity\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words='english',\n",
    "        max_features=10000,\n",
    "        ngram_range=(1, 2),         # Use unigrams + bigrams\n",
    "        min_df=5,                   # Remove rare terms\n",
    "        max_df=0.8,                 # Remove very frequent terms\n",
    "        sublinear_tf=True\n",
    "    )\n",
    "    X = vectorizer.fit_transform(transcripts + [user_input])\n",
    "\n",
    "    # Step 4: Similarity matching \n",
    "    # Compares the user input (last row) against all transcripts (first rows) using cosine similarity\n",
    "    similarities = cosine_similarity(X[-1], X[:-1]).flatten()\n",
    "\n",
    "    # Get indices of top 5 most similar transcripts\n",
    "    top_indices = similarities.argsort()[::-1][:top_n]\n",
    "\n",
    "    print(f\"\\nðŸ” Top {top_n} most relevant transcripts:\")\n",
    "\n",
    "    for rank, idx in enumerate(top_indices, start=1):\n",
    "        similarity_score = similarities[idx]\n",
    "        matched_filename = doc_names[idx]\n",
    "        matched_transcript = transcripts[idx]\n",
    "\n",
    "        print(f\"\\n{rank}. {matched_filename} (similarity: {similarity_score:.3f})\")\n",
    "\n",
    "        # Fetch episode details from df (assuming df is loaded and has 'Transcript' column matching filenames)\n",
    "        episode_df = df[df['Transcript'] == matched_filename]\n",
    "\n",
    "        if not episode_df.empty:\n",
    "            print(f\"   Title: {episode_df['title'].values[0]}\")\n",
    "            print(f\"   Link: {episode_df['webpage_url'].values[0]}\")\n",
    "            print(f\"   Duration: {episode_df['Duration'].values[0]} minutes\")\n",
    "            print(f\"   Uploaded on: {episode_df['Upload Date'].values[0]}\")\n",
    "        else:\n",
    "            print(\"   Episode details not found in dataframe.\")\n",
    "  \n",
    "    # print the matched terms with the user input\n",
    "    print(\"\\nðŸ” Matched terms with your input:\")\n",
    "    matched_terms = set(user_input.lower().split()) & set(matched_transcript.lower().split())\n",
    "    if matched_terms:\n",
    "        print(\", \".join(matched_terms))\n",
    "    else:\n",
    "        print(\"   No specific terms matched with your input.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbcd0305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Top 3 most relevant transcripts:\n",
      "\n",
      "1. 11__transcript.txt (similarity: 0.058)\n",
      "   Title: How to Stop Headaches Using Science-Based Approaches -- Huberman Lab Podcast\n",
      "   Link: https://www.youtube.com/watch?v=CGjdgy0cwGk\n",
      "   Duration: 146.73333333333332 minutes\n",
      "   Uploaded on: 06/02/23\n",
      "\n",
      "2. 26__transcript.txt (similarity: 0.055)\n",
      "   Title: Using Caffeine to Optimize Mental & Physical Performance -- Huberman Lab Podcast 101\n",
      "   Link: https://www.youtube.com/watch?v=iw97uvIge7c\n",
      "   Duration: 142.58333333333334 minutes\n",
      "   Uploaded on: 05/12/22\n",
      "\n",
      "3. 6__transcript.txt (similarity: 0.046)\n",
      "   Title: Dr. Andy Galpin: Optimal Nutrition & Supplementation for Fitness -- Huberman Lab Guest Series\n",
      "   Link: https://www.youtube.com/watch?v=q37ARYnRDGc\n",
      "   Duration: 185.55 minutes\n",
      "   Uploaded on: 22/02/23\n",
      "\n",
      "ðŸ” Matched terms with your input:\n",
      "can, reduce, what, fatigue, and, mood, supplements, like, help, symptoms\n"
     ]
    }
   ],
   "source": [
    "user_input = \"What supplements can help reduce PMS symptoms like fatigue and mood swings?\"\n",
    "recommend_transcripts(user_input, top_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1dbd26af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Top 3 most relevant transcripts:\n",
      "\n",
      "1. 126__transcript.txt (similarity: 0.037)\n",
      "   Title: Using Failures, Movement & Balance to Learn Faster -- Huberman Lab Podcast #7\n",
      "   Link: https://www.youtube.com/watch?v=hx3U64IXFOY\n",
      "   Duration: 88.08333333333333 minutes\n",
      "   Uploaded on: 15/02/21\n",
      "\n",
      "2. 123__transcript.txt (similarity: 0.033)\n",
      "   Title: Tools for Managing Stress & Anxiety -- Huberman Lab Podcast #10\n",
      "   Link: https://www.youtube.com/watch?v=ntfcfJ28eiU\n",
      "   Duration: 98.4 minutes\n",
      "   Uploaded on: 08/03/21\n",
      "\n",
      "3. 125__transcript.txt (similarity: 0.032)\n",
      "   Title: Optimize Your Learning & Creativity with Science-based Tools -- Huberman Lab Podcast #8\n",
      "   Link: https://www.youtube.com/watch?v=uuP-1ioh4LY\n",
      "   Duration: 90.58333333333333 minutes\n",
      "   Uploaded on: 22/02/21\n",
      "\n",
      "ðŸ” Matched terms with your input:\n",
      "am, feeling, tired, i\n"
     ]
    }
   ],
   "source": [
    "user_input =\"i am feeling tired\"\n",
    "recommend_transcripts(user_input, top_n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aafc824",
   "metadata": {},
   "source": [
    "# TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2805120f",
   "metadata": {},
   "source": [
    "# Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bd5ee5",
   "metadata": {},
   "source": [
    "### User Item rating matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "337187a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class CollaborativeFilteringRecommender:\n",
    "    def __init__(self, ratings_matrix):\n",
    "        self.ratings = ratings_matrix\n",
    "        self.user_user_similarity = cosine_similarity(self.ratings)\n",
    "        self.item_item_similarity = cosine_similarity(self.ratings.T)\n",
    "\n",
    "    def user_based_scores(self, user_id):\n",
    "        # Get similarity scores between the target user and all other users\n",
    "        sim_scores = self.user_user_similarity[user_id]\n",
    "\n",
    "        # Compute the weighted sum of all users' ratings, weighted by similarity\n",
    "        weighted_ratings = sim_scores @ self.ratings\n",
    "\n",
    "        # Compute the sum of similarities (excluding the target user's self-similarity)\n",
    "        sim_sum = np.sum(sim_scores) - 1\n",
    "        sim_sum = sim_sum if sim_sum != 0 else 1e-8  # Avoid division by zero\n",
    "\n",
    "        # Calculate predicted scores for each item by normalizing weighted ratings\n",
    "        # Subtract user's own ratings to reduce bias from already seen items\n",
    "        scores = (weighted_ratings - self.ratings[user_id]) / sim_sum\n",
    "\n",
    "        # Mark already-rated items with -1 so they are not recommended again\n",
    "        scores[self.ratings[user_id] > 0] = -1\n",
    "\n",
    "        # Return the predicted scores for unrated items\n",
    "        return scores\n",
    "\n",
    "\n",
    "    def item_based_scores(self, user_id):\n",
    "        scores = np.zeros(self.ratings.shape[1])\n",
    "        user_ratings = self.ratings[user_id]\n",
    "\n",
    "        for item in range(self.ratings.shape[1]):\n",
    "            if user_ratings[item] != 0:\n",
    "                scores[item] = -1 # Already rated\n",
    "                continue\n",
    "            \n",
    "            # Similarities of current item to all other items\n",
    "            sim_scores = self.item_item_similarity[item]\n",
    "            \n",
    "            # Ratings of the user for all other items\n",
    "            rated_mask = user_ratings > 0\n",
    "\n",
    "            # Weighted sum of similar items' ratings\n",
    "            weighted_sum = np.dot(sim_scores, user_ratings)\n",
    "            \n",
    "            # Remove self-similarity contribution\n",
    "            sim_sum = np.sum(sim_scores[rated_mask])\n",
    "            sim_sum = sim_sum if sim_sum != 0 else 1e-8\n",
    "\n",
    "            scores[item] = weighted_sum / sim_sum\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def hybrid_scores(self, user_id, alpha=0.7):\n",
    "        \"\"\"alpha=0.5 means equal weight; alpha=1 uses only user-based\"\"\"\n",
    "        user_scores = self.user_based_scores(user_id)\n",
    "        item_scores = self.item_based_scores(user_id)\n",
    "\n",
    "        combined_scores = alpha * user_scores + (1 - alpha) * item_scores\n",
    "        # Preserve -1 for already rated items\n",
    "        combined_scores[self.ratings[user_id] > 0] = -1\n",
    "        return combined_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fb14422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Health', 'AI', 'Startups', 'Productivity', 'Nutrition']\n",
      "[0, 1, 2, 3, 4]\n",
      "User-based CF scores:\n",
      "[-1.         -1.          0.97079276 -1.          0.90312423]\n",
      "\n",
      "Item-based CF scores:\n",
      "[-1.         -1.          1.44900412 -1.          2.10253074]\n",
      "\n",
      "Hybrid CF scores (alpha=0.7):\n",
      "[-1.         -1.          1.11425617 -1.          1.26294618]\n",
      "----------------------\n",
      "User-based CF Recommendations:\n",
      "  Startups: 0.971\n",
      "  Nutrition: 0.903\n",
      "\n",
      "Item-based CF Recommendations:\n",
      "  Nutrition: 2.103\n",
      "  Startups: 1.449\n",
      "\n",
      "Hybrid CF Recommendations (alpha=0.7):\n",
      "  Nutrition: 1.263\n",
      "  Startups: 1.114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# User-Item rating matrix (rows = users, columns = items)\n",
    "# 0 means no rating\n",
    "\n",
    "ratings = np.array([\n",
    "    [5, 3, 0, 1, 0],  # User 0  \n",
    "    [4, 0, 0, 1, 0],  # User 1 \n",
    "    [1, 1, 0, 5, 4],  # User 2  \n",
    "    [0, 0, 5, 4, 0],  # User 3\n",
    "    [0, 1, 5, 4, 0],  # User 4\n",
    "])\n",
    "\n",
    "item_ids = [\"Health\", \"AI\", \"Startups\", \"Productivity\", \"Nutrition\"]\n",
    "user_ids = [0,1,2,3,4]\n",
    "\n",
    "recommender = CollaborativeFilteringRecommender(ratings)\n",
    "\n",
    "user_id = 0\n",
    "print(item_ids)\n",
    "print(user_ids)\n",
    "print(\"User-based CF scores:\")\n",
    "user_based = recommender.user_based_scores(user_id)\n",
    "print(user_based)\n",
    "\n",
    "print(\"\\nItem-based CF scores:\")\n",
    "item_based = recommender.item_based_scores(user_id)\n",
    "print(item_based)\n",
    "\n",
    "alpha=0.7\n",
    "print(f\"\\nHybrid CF scores (alpha={alpha}):\")\n",
    "hybrid_based = recommender.hybrid_scores(user_id, alpha=alpha)\n",
    "print(hybrid_based)\n",
    "\n",
    "def print_recommendations(title, scores, item_ids, top_k=3):\n",
    "    print(title)\n",
    "    # Collect only unrated (recommended) items\n",
    "    recommendations = [(item_ids[i], round(scores[i], 3)) for i in range(len(scores)) if scores[i] > 0]\n",
    "    recommendations.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    if not recommendations:\n",
    "        print(\"  No recommendations.\")\n",
    "    else:\n",
    "        for item, score in recommendations[:top_k]:\n",
    "            print(f\"  {item}: {score}\")\n",
    "    print()\n",
    "\n",
    "print(\"----------------------\")\n",
    "print_recommendations(\"User-based CF Recommendations:\", user_based, item_ids)\n",
    "print_recommendations(\"Item-based CF Recommendations:\", item_based, item_ids)\n",
    "print_recommendations(f\"Hybrid CF Recommendations (alpha={alpha}):\", hybrid_based, item_ids)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c418d9be",
   "metadata": {},
   "source": [
    "# Collaborative + Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "991c566f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class HybridRecommender:\n",
    "    def __init__(self, ratings_matrix, item_features, w_user=0.4, w_item=0.4, w_content=0.2):\n",
    "        self.ratings = ratings_matrix\n",
    "        self.user_user_similarity = cosine_similarity(self.ratings)\n",
    "        self.item_item_similarity = cosine_similarity(self.ratings.T)\n",
    "        \n",
    "        self.item_features = item_features\n",
    "        self.w_user = w_user\n",
    "        self.w_item = w_item\n",
    "        self.w_content = w_content\n",
    "\n",
    "    def user_based_scores(self, user_id):\n",
    "        # Get similarity scores between the target user and all other users\n",
    "        sim_scores = self.user_user_similarity[user_id]\n",
    "\n",
    "        # Compute the weighted sum of all users' ratings, weighted by similarity\n",
    "        weighted_ratings = sim_scores @ self.ratings\n",
    "\n",
    "        # Compute the sum of similarities (excluding the target user's self-similarity)\n",
    "        sim_sum = np.sum(sim_scores) - 1\n",
    "        sim_sum = sim_sum if sim_sum != 0 else 1e-8  # Avoid division by zero\n",
    "\n",
    "        # Calculate predicted scores for each item by normalizing weighted ratings\n",
    "        # Subtract user's own ratings to reduce bias from already seen items\n",
    "        scores = (weighted_ratings - self.ratings[user_id]) / sim_sum\n",
    "\n",
    "        # Mark already-rated items with -1 so they are not recommended again\n",
    "        scores[self.ratings[user_id] > 0] = -1\n",
    "\n",
    "        # Return the predicted scores for unrated items\n",
    "        return scores\n",
    "\n",
    "\n",
    "    def item_based_scores(self, user_id):\n",
    "        scores = np.zeros(self.ratings.shape[1])\n",
    "        user_ratings = self.ratings[user_id]\n",
    "\n",
    "        for item in range(self.ratings.shape[1]):\n",
    "            if user_ratings[item] != 0:\n",
    "                scores[item] = -1 # Already rated\n",
    "                continue\n",
    "            \n",
    "            # Similarities of current item to all other items\n",
    "            sim_scores = self.item_item_similarity[item]\n",
    "            \n",
    "            # Ratings of the user for all other items\n",
    "            rated_mask = user_ratings > 0\n",
    "\n",
    "            # Weighted sum of similar items' ratings\n",
    "            weighted_sum = np.dot(sim_scores, user_ratings)\n",
    "            \n",
    "            # Remove self-similarity contribution\n",
    "            sim_sum = np.sum(sim_scores[rated_mask])\n",
    "            sim_sum = sim_sum if sim_sum != 0 else 1e-8\n",
    "\n",
    "            scores[item] = weighted_sum / sim_sum\n",
    "\n",
    "        return scores\n",
    "    \n",
    "\n",
    "    def collaborative_hybrid_scores(self, user_id, alpha=0.7):\n",
    "        \"\"\"alpha=0.5 means equal weight; alpha=1 uses only user-based\"\"\"\n",
    "        user_scores = self.user_based_scores(user_id)\n",
    "        item_scores = self.item_based_scores(user_id)\n",
    "\n",
    "        combined_scores = alpha * user_scores + (1 - alpha) * item_scores\n",
    "        # Preserve -1 for already rated items\n",
    "        combined_scores[self.ratings[user_id] > 0] = -1\n",
    "        \n",
    "        recommended_idx = np.argsort(combined_scores)[::-1]\n",
    "\n",
    "        return [(idx, combined_scores[idx]) for idx in recommended_idx]\n",
    "    \n",
    "    \n",
    "    def content_based_scores(self, user_id):\n",
    "        user_ratings = self.ratings[user_id]\n",
    "        user_profile = user_ratings @ self.item_features\n",
    "        item_norm = np.linalg.norm(self.item_features, axis=1)\n",
    "        profile_norm = np.linalg.norm(user_profile)\n",
    "        scores = (self.item_features @ user_profile) / (item_norm * profile_norm + 1e-8)\n",
    "        scores[user_ratings > 0] = -1\n",
    "       \n",
    "        return scores\n",
    "    \n",
    "    \n",
    "    def content_hybrid_scores(self, user_id):\n",
    "        user_scores = self.user_based_scores(user_id)\n",
    "        item_scores = self.item_based_scores(user_id)\n",
    "        content_scores = self.content_based_scores(user_id)\n",
    "\n",
    "        combined_scores = (self.w_user * user_scores +\n",
    "                           self.w_item * item_scores +\n",
    "                           self.w_content * content_scores)\n",
    "\n",
    "        recommended_idx = np.argsort(combined_scores)[::-1]\n",
    "\n",
    "        return [(idx, combined_scores[idx]) for idx in recommended_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a099d848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hybrid ecommendations for user 0:\n",
      "Item 4 with combined score 1.356\n",
      "Item 2 with combined score 1.025\n",
      "Item 3 with combined score -1.000\n",
      "Item 1 with combined score -1.000\n",
      "Item 0 with combined score -1.000\n",
      "--------------\n",
      "collaborative ecommendations for user 0:\n",
      "Item 4 with combined score 1.263\n",
      "Item 2 with combined score 1.114\n",
      "Item 3 with combined score -1.000\n",
      "Item 1 with combined score -1.000\n",
      "Item 0 with combined score -1.000\n"
     ]
    }
   ],
   "source": [
    "ratings = np.array([\n",
    "    [5, 3, 0, 1, 0],  # User 0  \n",
    "    [4, 0, 0, 1, 0],  # User 1 \n",
    "    [1, 1, 0, 5, 4],  # User 2  \n",
    "    [0, 0, 5, 4, 0],  # User 3\n",
    "    [0, 1, 5, 4, 0],  # User 4\n",
    "])\n",
    "\n",
    "item_ids = [\"Health\", \"AI\", \"Startups\", \"Productivity\", \"Nutrition\"]\n",
    "user_ids = [0,1,2,3,4]\n",
    "\n",
    "\n",
    "item_features = np.array([\n",
    "    [1, 0, 1],  # Action + Drama\n",
    "    [1, 1, 0],  # Action + Comedy\n",
    "    [0, 1, 0],  # Comedy\n",
    "    [0, 0, 1],  # Drama\n",
    "    [1, 0, 0],  # Action\n",
    "])\n",
    "\n",
    "recommender = HybridRecommender(ratings, item_features)\n",
    "user_id = 0\n",
    "hybrid_recommendations = recommender.content_hybrid_scores(user_id)\n",
    "collaborative_recommendations = recommender.collaborative_hybrid_scores(user_id)\n",
    "\n",
    "print(f\"hybrid ecommendations for user {user_id}:\")\n",
    "for idx, score in hybrid_recommendations:\n",
    "    print(f\"Item {idx} with combined score {score:.3f}\")\n",
    "    \n",
    "print(\"--------------\")\n",
    "print(f\"collaborative ecommendations for user {user_id}:\")\n",
    "for idx, score in collaborative_recommendations:\n",
    "    print(f\"Item {idx} with combined score {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3bb5fb",
   "metadata": {},
   "source": [
    "# Using Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2513c0",
   "metadata": {},
   "source": [
    "Step 1: Build the user-item rating matrix\n",
    "\n",
    "Each row is a user, each column is a podcast, and values are ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086bc9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "user_rest_matrix = ratings.pivot(index='user_id', columns='podcast_id', values='rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a56579",
   "metadata": {},
   "source": [
    "### User based CF\n",
    "\n",
    "Use cosine similarity or Pearson correlation between user rating vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3006d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN with 0 for similarity (or use mean-centered if Pearson)\n",
    "user_similarity = cosine_similarity(user_rest_matrix.fillna(0))\n",
    "user_similarity_df = pd.DataFrame(user_similarity, \n",
    "                                   index=user_rest_matrix.index,\n",
    "                                   columns=user_rest_matrix.index)\n",
    "\n",
    "\n",
    "# Step 3: Make predictions\n",
    "# To predict how much user A would like podcast X:\n",
    "# Find k most similar users to A\n",
    "# Average their ratings for X, weighted by similarity\n",
    "\n",
    "def predict_rating_user_based(target_user, target_item, k=5):\n",
    "    similar_users = user_similarity_df[target_user].drop(target_user).sort_values(ascending=False).head(k)\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    for other_user, similarity in similar_users.items():\n",
    "        rating = user_rest_matrix.loc[other_user, target_item]\n",
    "        if not pd.isna(rating):\n",
    "            numerator += similarity * rating\n",
    "            denominator += abs(similarity)\n",
    "    return numerator / denominator if denominator != 0 else np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be7ce80",
   "metadata": {},
   "source": [
    "### Item based CF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180d175c",
   "metadata": {},
   "source": [
    "Step 1: Transpose the user-item matrix\n",
    "\n",
    "Now rows = podcasts, columns = users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0266a851",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_user_matrix = user_rest_matrix.T  # rows = podcast_id\n",
    "\n",
    "item_similarity = cosine_similarity(item_user_matrix.fillna(0))\n",
    "item_similarity_df = pd.DataFrame(item_similarity, \n",
    "                                   index=item_user_matrix.index,\n",
    "                                   columns=item_user_matrix.index)\n",
    "\n",
    "\n",
    "# Step 3: Predict rating\n",
    "\n",
    "# To predict how much user U would like podcast R:\n",
    "\n",
    "# Look at items R is similar to\n",
    "# Use Uâ€™s ratings on those similar items, weighted by similarity\n",
    "\n",
    "def predict_rating_item_based(user_id, target_item, k=5):\n",
    "    similar_items = item_similarity_df[target_item].drop(target_item).sort_values(ascending=False)\n",
    "    user_ratings = user_rest_matrix.loc[user_id]\n",
    "    \n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    for item_id, similarity in similar_items.head(k).items():\n",
    "        if not pd.isna(user_ratings.get(item_id)):\n",
    "            numerator += similarity * user_ratings[item_id]\n",
    "            denominator += abs(similarity)\n",
    "    return numerator / denominator if denominator != 0 else np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873ec0c8",
   "metadata": {},
   "source": [
    "# Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b38854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af623e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate duplicate ratings by taking the mean\n",
    "df_agg = df.groupby(['userId', 'title'])['rating'].mean().reset_index()\n",
    "\n",
    "user_item_matrix = df_agg.pivot(index='userId', columns='title', values='rating')\n",
    "\n",
    "user_item_matrix.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff5dffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select a user\n",
    "user_id = 208\n",
    "\n",
    "# Get podcasts watched by this user\n",
    "user_podcasts = user_item_matrix.loc[user_id].dropna().index.tolist()\n",
    "print(f\"User {user_id} watched {len(user_podcasts)} movies\")\n",
    "\n",
    "# Keep only these movies for all users\n",
    "subset_matrix = user_item_matrix[user_podcasts]\n",
    "\n",
    "# Keep only users who watched at least 70% of these movies\n",
    "min_podcasts = int(len(user_podcasts) * 0.7)\n",
    "users_to_keep = subset_matrix.count(axis=1) >= min_podcasts\n",
    "filtered_matrix = subset_matrix[users_to_keep]\n",
    "\n",
    "print(f\"Kept {len(filtered_matrix)} users that watched at least {min_podcasts} podcasts\")\n",
    "\n",
    "# Calculate user similarities\n",
    "user_correlations = filtered_matrix.T.corr()\n",
    "\n",
    "# Get similarities for our target user (excluding self)\n",
    "similar_users = user_correlations[user_id].drop(user_id).sort_values(ascending=False)\n",
    "\n",
    "print(f\"\\nTop 10 most similar users to user {user_id}:\")\n",
    "similar_users.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9261cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get movie recommendations\n",
    "def user_based_recommendation(user_id, n_recommendations=5):\n",
    "    \n",
    "    # Get top 5 similar users\n",
    "    top_similar = similar_users.head(10)\n",
    "    \n",
    "    # podcast target user hasn't watched\n",
    "    unwatched_podcasts = user_item_matrix.columns[user_item_matrix.loc[user_id].isna()]\n",
    "    \n",
    "    # Score each unwatched movie\n",
    "    podcast_scores = {}\n",
    "    similarities_sum = top_similar.sum()\n",
    "    if similarities_sum == 0:\n",
    "        print(\"No similar users found or all similarities are zero.\")\n",
    "        return []\n",
    "    \n",
    "     # Add target average rating to the score\n",
    "    target_user_avg = user_item_matrix.loc[user_id].mean()\n",
    "    \n",
    "    for podcast in unwatched_podcasts:\n",
    "        \n",
    "        # calculate summation of scores\n",
    "        score = 0\n",
    "        for similar_user, similarity in top_similar.items():\n",
    "            if not pd.isna(user_item_matrix.loc[similar_user, podcast]):\n",
    "                rating = user_item_matrix.loc[similar_user, podcast]\n",
    "                centered_rating = rating - user_item_matrix.loc[similar_user].mean()\n",
    "                score += centered_rating * similarity\n",
    "                \n",
    "        podcast_scores[podcast] = (score + target_user_avg) / similarities_sum\n",
    "        \n",
    "    # Return top recommendations\n",
    "    recommendations = sorted(podcast_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return recommendations[:n_recommendations]\n",
    "\n",
    "# Get recommendations\n",
    "recommendations = user_based_recommendation(user_id)\n",
    "print(f\"\\nTop 5 recommendations for user {user_id}:\")\n",
    "for i, (podcast, score) in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. Podcast {podcast}: Score = {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e10e5b7",
   "metadata": {},
   "source": [
    "# SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9318d231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=500, random_state=42)\n",
    "R = filtered_matrix.fillna(0)\n",
    "svd.fit(R)  # R is user-item matrix with NaNs filled as 0\n",
    "\n",
    "cumulative_variance = np.cumsum(svd.explained_variance_ratio_)\n",
    "\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o')\n",
    "plt.axhline(y=0.9, color='r', linestyle='--')  # for 90% variance explained\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Choosing Number of Components for SVD')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b230ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "for k in [10, 20, 50, 100, 200, 500]:\n",
    "    svd = TruncatedSVD(n_components=k, random_state=42)\n",
    "    U = svd.fit_transform(R)\n",
    "    Vt = svd.components_\n",
    "    R_hat = np.dot(U, Vt)\n",
    "    mse = mean_squared_error(R.flatten(), R_hat.flatten())\n",
    "    print(f\"k={k}, RMSE={np.sqrt(mse):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ce621b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Assuming df_agg and user_item_matrix are ready\n",
    "R = user_item_matrix.fillna(0).values\n",
    "k = 500\n",
    "\n",
    "# Perform SVD using scikit-learn\n",
    "svd = TruncatedSVD(n_components=k, random_state=42)\n",
    "U = svd.fit_transform(R)  # shape: (n_users, k)\n",
    "sigma = svd.singular_values_  # shape: (k,)\n",
    "Vt = svd.components_          # shape: (k, n_items)\n",
    "\n",
    "# Reconstruct the approximate user-item matrix\n",
    "R_hat = np.dot(U, Vt)  # since TruncatedSVD includes the scaling in U and Vt\n",
    "\n",
    "# Create prediction DataFrame\n",
    "preds_df = pd.DataFrame(R_hat, index=user_item_matrix.index, columns=user_item_matrix.columns)\n",
    "\n",
    "def recommend_podcasts(preds_df, user_id, user_item_matrix, top_n=5):\n",
    "    # Get user's predicted ratings\n",
    "    user_row = preds_df.loc[user_id]\n",
    "\n",
    "    # Get movies already rated by user\n",
    "    rated_podcasts = user_item_matrix.loc[user_id].dropna().index.tolist()\n",
    "\n",
    "    # Filter out movies already rated\n",
    "    recommendations = user_row.drop(labels=rated_podcasts)\n",
    "\n",
    "    # Get top N recommendations\n",
    "    top_recommendations = recommendations.sort_values(ascending=False).head(top_n)\n",
    "\n",
    "    return top_recommendations\n",
    "\n",
    "# Select a random user_id (make sure the ID exists in the index)\n",
    "user_id = 220\n",
    "\n",
    "recommendations = recommend_podcasts(preds_df, user_id, user_item_matrix, top_n=5)\n",
    "\n",
    "print(f\"Top recommendations for user {user_id}:\")\n",
    "print(recommendations)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
